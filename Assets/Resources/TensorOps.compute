// ============================================================
//  ChaosRL — Tensor Operations Compute Shader
//  All GPU kernels for ITensorBackend in a single file.
// ============================================================

#define THREADS 256
#define TILE    16

// ============================================================
//  Uniforms (set from GpuBackend before each dispatch)
// ============================================================

uint _Size;
uint _SizeA;
uint _SizeB;
uint _ResultSize;
float _Exponent;
float _MinVal;
float _MaxVal;
float _AGradScale;
float _BGradScale;

// MatMul / Transpose dimensions
uint _M;
uint _K;
uint _N;
uint _Accumulate;

// Reduction dimension decomposition
uint _OuterSize;
uint _DimSize;
uint _InnerSize;

// Slice / Copy
uint _SrcBlockSize;
uint _DstBlockSize;
uint _StartOffset;
uint _SrcOffset;
uint _DstOffset;
uint _Count;

// ExpandLast / Gather
uint _Num;
uint _FeatureSize;

// Adam optimizer
uint _MomentOffset;
float _LR;
float _Beta1;
float _Beta2;
float _Epsilon;
float _InvBias1;
float _InvBias2;

// Fill
float _FillValue;

// ============================================================
//  Buffer declarations
// ============================================================

// --- Forward binary / unary ---
StructuredBuffer<float>   _A;
StructuredBuffer<float>   _B;
RWStructuredBuffer<float> _Result;

// --- Unary input ---
StructuredBuffer<float>   _Input;

// --- Backward binary (uint for CAS-based atomic float add) ---
StructuredBuffer<float>   _AData;
StructuredBuffer<float>   _BData;
RWStructuredBuffer<uint>  _AGrad;
RWStructuredBuffer<uint>  _BGrad;
StructuredBuffer<float>   _ResultGrad;

// --- Backward unary ---
StructuredBuffer<float>   _InputData;
StructuredBuffer<float>   _ResultData;
RWStructuredBuffer<float> _InputGrad;

// --- Reduction output ---
RWStructuredBuffer<float> _Output;
RWStructuredBuffer<uint>  _MaxIdxBuf;

// --- Data movement ---
StructuredBuffer<float>   _Src;
RWStructuredBuffer<float> _Dst;

// --- Gather ---
StructuredBuffer<uint>    _Indices;

// --- Adam ---
RWStructuredBuffer<float> _Data;
StructuredBuffer<float>   _Grad;
RWStructuredBuffer<float> _MBuf;
RWStructuredBuffer<float> _VBuf;

// ============================================================
//  Float atomic add via CAS loop (for broadcast backward)
// ============================================================

void FloatAtomicAdd( RWStructuredBuffer<uint> buf, uint idx, float val )
{
    uint expected;
    uint original;
    [allow_uav_condition]
    do
    {
        expected = buf[ idx ];
        uint desired = asuint( asfloat( expected ) + val );
        InterlockedCompareExchange( buf[ idx ], expected, desired, original );
    }
    while ( original != expected );
}

// ============================================================
//  Element-wise binary — forward
// ============================================================

#pragma kernel ElementAdd
[numthreads( THREADS, 1, 1 )]
void ElementAdd( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    _Result[ i ] = _A[ i % _SizeA ] + _B[ i % _SizeB ];
}

//------------------------------------------------------------------
#pragma kernel ElementMul
[numthreads( THREADS, 1, 1 )]
void ElementMul( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    _Result[ i ] = _A[ i % _SizeA ] * _B[ i % _SizeB ];
}

//------------------------------------------------------------------
#pragma kernel ElementDiv
[numthreads( THREADS, 1, 1 )]
void ElementDiv( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    _Result[ i ] = _A[ i % _SizeA ] / _B[ i % _SizeB ];
}

//------------------------------------------------------------------
#pragma kernel ElementMax
[numthreads( THREADS, 1, 1 )]
void ElementMax( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float a = _A[ i % _SizeA ];
    float b = _B[ i % _SizeB ];
    _Result[ i ] = ( a >= b ) ? a : b;
}

//------------------------------------------------------------------
#pragma kernel ElementMin
[numthreads( THREADS, 1, 1 )]
void ElementMin( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float a = _A[ i % _SizeA ];
    float b = _B[ i % _SizeB ];
    _Result[ i ] = ( a <= b ) ? a : b;
}

// ============================================================
//  Element-wise binary — backward  (atomic float add)
// ============================================================

#pragma kernel AddBackward
[numthreads( THREADS, 1, 1 )]
void AddBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    if ( _AGradScale != 0.0 )
        FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg );
    if ( _BGradScale != 0.0 )
        FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * rg );
}

//------------------------------------------------------------------
#pragma kernel MulBackward
[numthreads( THREADS, 1, 1 )]
void MulBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( _AGradScale != 0.0 )
        FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * bd * rg );
    if ( _BGradScale != 0.0 )
        FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * ad * rg );
}

//------------------------------------------------------------------
#pragma kernel DivBackward
[numthreads( THREADS, 1, 1 )]
void DivBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( _AGradScale != 0.0 )
        FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg / bd );
    if ( _BGradScale != 0.0 )
        FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * ( -ad / ( bd * bd ) ) * rg );
}

//------------------------------------------------------------------
#pragma kernel ElementMaxBackward
[numthreads( THREADS, 1, 1 )]
void ElementMaxBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( ad >= bd )
    {
        if ( _AGradScale != 0.0 )
            FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg );
    }
    else
    {
        if ( _BGradScale != 0.0 )
            FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * rg );
    }
}

//------------------------------------------------------------------
#pragma kernel ElementMinBackward
[numthreads( THREADS, 1, 1 )]
void ElementMinBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( ad <= bd )
    {
        if ( _AGradScale != 0.0 )
            FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg );
    }
    else
    {
        if ( _BGradScale != 0.0 )
            FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * rg );
    }
}

// ============================================================
//  Element-wise unary — forward
// ============================================================

#pragma kernel Pow
[numthreads( THREADS, 1, 1 )]
void Pow( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = pow( abs( _Input[ i ] ), _Exponent ) * sign( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel Exp
[numthreads( THREADS, 1, 1 )]
void Exp( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = exp( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel Log
[numthreads( THREADS, 1, 1 )]
void Log( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = log( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel ReLU
[numthreads( THREADS, 1, 1 )]
void ReLU( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float v = _Input[ i ];
    _Result[ i ] = ( v > 0.0 ) ? v : 0.0;
}

//------------------------------------------------------------------
#pragma kernel Tanh
[numthreads( THREADS, 1, 1 )]
void Tanh( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = tanh( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel Clamp
[numthreads( THREADS, 1, 1 )]
void Clamp( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = clamp( _Input[ i ], _MinVal, _MaxVal );
}

// ============================================================
//  Element-wise unary — backward
// ============================================================

#pragma kernel PowBackward
[numthreads( THREADS, 1, 1 )]
void PowBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float x = _InputData[ i ];
    float rg = _ResultGrad[ i ];
    _InputGrad[ i ] += _Exponent * pow( abs( x ), _Exponent - 1.0 ) * sign( x ) * rg;
}

//------------------------------------------------------------------
#pragma kernel ExpBackward
[numthreads( THREADS, 1, 1 )]
void ExpBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _InputGrad[ i ] += _ResultData[ i ] * _ResultGrad[ i ];
}

//------------------------------------------------------------------
#pragma kernel LogBackward
[numthreads( THREADS, 1, 1 )]
void LogBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _InputGrad[ i ] += _ResultGrad[ i ] / _InputData[ i ];
}

//------------------------------------------------------------------
#pragma kernel ReLUBackward
[numthreads( THREADS, 1, 1 )]
void ReLUBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _InputGrad[ i ] += ( _InputData[ i ] > 0.0 ? 1.0 : 0.0 ) * _ResultGrad[ i ];
}

//------------------------------------------------------------------
#pragma kernel TanhBackward
[numthreads( THREADS, 1, 1 )]
void TanhBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float t = _ResultData[ i ];
    _InputGrad[ i ] += ( 1.0 - t * t ) * _ResultGrad[ i ];
}

//------------------------------------------------------------------
#pragma kernel ClampBackward
[numthreads( THREADS, 1, 1 )]
void ClampBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float x = _InputData[ i ];
    _InputGrad[ i ] += ( x >= _MinVal && x <= _MaxVal ? 1.0 : 0.0 ) * _ResultGrad[ i ];
}

// ============================================================
//  Reductions
// ============================================================

groupshared float _sharedF[ THREADS ];
groupshared uint  _sharedU[ THREADS ];

//------------------------------------------------------------------
#pragma kernel SumReduce
[numthreads( THREADS, 1, 1 )]
void SumReduce( uint3 dtid : SV_DispatchThreadID,
                uint3 gtid : SV_GroupThreadID,
                uint3 gid  : SV_GroupID )
{
    uint tid = gtid.x;
    _sharedF[ tid ] = ( dtid.x < _Size ) ? _Input[ dtid.x ] : 0.0;
    GroupMemoryBarrierWithGroupSync();

    for ( uint s = THREADS / 2; s > 0; s >>= 1 )
    {
        if ( tid < s )
            _sharedF[ tid ] += _sharedF[ tid + s ];
        GroupMemoryBarrierWithGroupSync();
    }

    if ( tid == 0 )
        _Output[ gid.x ] = _sharedF[ 0 ];
}

//------------------------------------------------------------------
#pragma kernel SumBackward
[numthreads( THREADS, 1, 1 )]
void SumBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    // _Output[0] holds the upstream scalar gradient; broadcast to all input grad slots
    _InputGrad[ i ] += _Output[ 0 ];
}

//------------------------------------------------------------------
#pragma kernel SumDim
[numthreads( THREADS, 1, 1 )]
void SumDim( uint3 id : SV_DispatchThreadID )
{
    // Each thread handles one (outer, inner) pair
    uint oi = id.x;
    if ( oi >= _OuterSize * _InnerSize ) return;

    uint outer = oi / _InnerSize;
    uint inner = oi % _InnerSize;

    float acc = 0.0;
    uint base_idx = outer * _DimSize * _InnerSize + inner;
    for ( uint d = 0; d < _DimSize; d++ )
        acc += _Input[ base_idx + d * _InnerSize ];

    _Output[ oi ] = acc;
}

//------------------------------------------------------------------
#pragma kernel SumDimBackward
[numthreads( THREADS, 1, 1 )]
void SumDimBackward( uint3 id : SV_DispatchThreadID )
{
    uint oi = id.x;
    if ( oi >= _OuterSize * _InnerSize ) return;

    uint outer = oi / _InnerSize;
    uint inner = oi % _InnerSize;

    float g = _ResultGrad[ oi ];
    uint base_idx = outer * _DimSize * _InnerSize + inner;
    for ( uint d = 0; d < _DimSize; d++ )
        _InputGrad[ base_idx + d * _InnerSize ] += g;
}

//------------------------------------------------------------------
#pragma kernel MaxReduce
[numthreads( THREADS, 1, 1 )]
void MaxReduce( uint3 dtid : SV_DispatchThreadID,
                uint3 gtid : SV_GroupThreadID,
                uint3 gid  : SV_GroupID )
{
    uint tid = gtid.x;
    if ( dtid.x < _Size )
    {
        _sharedF[ tid ] = _Input[ dtid.x ];
        _sharedU[ tid ] = dtid.x;
    }
    else
    {
        _sharedF[ tid ] = -3.402823e+38;
        _sharedU[ tid ] = 0;
    }
    GroupMemoryBarrierWithGroupSync();

    for ( uint s = THREADS / 2; s > 0; s >>= 1 )
    {
        if ( tid < s )
        {
            if ( _sharedF[ tid + s ] > _sharedF[ tid ] )
            {
                _sharedF[ tid ] = _sharedF[ tid + s ];
                _sharedU[ tid ] = _sharedU[ tid + s ];
            }
            else if ( _sharedF[ tid + s ] == _sharedF[ tid ] && _sharedU[ tid + s ] < _sharedU[ tid ] )
            {
                // Tie-breaker: prefer the smaller index (first occurrence)
                _sharedU[ tid ] = _sharedU[ tid + s ];
            }
        }
        GroupMemoryBarrierWithGroupSync();
    }

    if ( tid == 0 )
    {
        _Output[ gid.x ]     = _sharedF[ 0 ];
        _MaxIdxBuf[ gid.x ]  = _sharedU[ 0 ];
    }
}

//------------------------------------------------------------------
#pragma kernel MaxReduceBackward
[numthreads( 1, 1, 1 )]
void MaxReduceBackward( uint3 id : SV_DispatchThreadID )
{
    // Single-element: gradient flows only to the max position
    uint idx = _MaxIdxBuf[ 0 ];
    _InputGrad[ idx ] += _Output[ 0 ];
}

//------------------------------------------------------------------
#pragma kernel MaxReduceDim
[numthreads( THREADS, 1, 1 )]
void MaxReduceDim( uint3 id : SV_DispatchThreadID )
{
    uint oi = id.x;
    if ( oi >= _OuterSize * _InnerSize ) return;

    uint outer = oi / _InnerSize;
    uint inner = oi % _InnerSize;

    uint blockSize = _DimSize * _InnerSize;
    uint base_idx = outer * blockSize + inner;

    float maxVal = -3.402823e+38;
    uint  maxI   = 0;
    for ( uint d = 0; d < _DimSize; d++ )
    {
        uint idx = base_idx + d * _InnerSize;
        float v = _Input[ idx ];
        if ( v > maxVal )
        {
            maxVal = v;
            maxI   = idx;
        }
    }

    _Output[ oi ]     = maxVal;
    _MaxIdxBuf[ oi ]  = maxI;
}

//------------------------------------------------------------------
#pragma kernel MaxReduceDimBackward
[numthreads( THREADS, 1, 1 )]
void MaxReduceDimBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    uint srcIdx = _MaxIdxBuf[ i ];
    _InputGrad[ srcIdx ] += _ResultGrad[ i ];
}

// ============================================================
//  MatMul
// ============================================================

#define TILE_M 32
#define TILE_N 32
#define TILE_K 16

groupshared float _As[ TILE_M ][ TILE_K ];
groupshared float _Bs[ TILE_K ][ TILE_N ];

#pragma kernel MatMulForward
// Threadgroup: 8x16 = 128 threads
// - X dimension handles columns in chunks of 4 (8*4 = 32 cols)
// - Y dimension handles rows in chunks of 2 (16*2 = 32 rows)
[numthreads( 8, 16, 1 )]
void MatMulForward( uint3 gid : SV_GroupID,
                    uint3 tid : SV_GroupThreadID )
{
    uint tx = tid.x; // 0..7
    uint ty = tid.y; // 0..15

    uint baseRow = gid.y * TILE_M;
    uint baseCol = gid.x * TILE_N;

    uint row0 = baseRow + ty * 2;
    uint row1 = row0 + 1;

    uint col0 = baseCol + tx * 4;
    uint col1 = col0 + 1;
    uint col2 = col0 + 2;
    uint col3 = col0 + 3;

    float c00 = 0.0, c01 = 0.0, c02 = 0.0, c03 = 0.0;
    float c10 = 0.0, c11 = 0.0, c12 = 0.0, c13 = 0.0;

    for ( uint k0 = 0; k0 < _K; k0 += TILE_K )
    {
        // --- Load A tile (32x16) ---
        // 128 threads, each loads 4 elements (2 rows x 2 cols)
        uint aCol0 = k0 + tx;
        uint aCol1 = k0 + tx + 8;

        if ( row0 < _M && aCol0 < _K ) _As[ ty * 2 + 0 ][ tx ] = _A[ row0 * _K + aCol0 ];
        else                           _As[ ty * 2 + 0 ][ tx ] = 0.0;

        if ( row1 < _M && aCol0 < _K ) _As[ ty * 2 + 1 ][ tx ] = _A[ row1 * _K + aCol0 ];
        else                           _As[ ty * 2 + 1 ][ tx ] = 0.0;

        if ( row0 < _M && aCol1 < _K ) _As[ ty * 2 + 0 ][ tx + 8 ] = _A[ row0 * _K + aCol1 ];
        else                           _As[ ty * 2 + 0 ][ tx + 8 ] = 0.0;

        if ( row1 < _M && aCol1 < _K ) _As[ ty * 2 + 1 ][ tx + 8 ] = _A[ row1 * _K + aCol1 ];
        else                           _As[ ty * 2 + 1 ][ tx + 8 ] = 0.0;

        // --- Load B tile (16x32) ---
        // Each thread loads 4 contiguous floats from a single B row
        uint bRow = k0 + ty;

        if ( bRow < _K )
        {
            _Bs[ ty ][ tx * 4 + 0 ] = ( col0 < _N ) ? _B[ bRow * _N + col0 ] : 0.0;
            _Bs[ ty ][ tx * 4 + 1 ] = ( col1 < _N ) ? _B[ bRow * _N + col1 ] : 0.0;
            _Bs[ ty ][ tx * 4 + 2 ] = ( col2 < _N ) ? _B[ bRow * _N + col2 ] : 0.0;
            _Bs[ ty ][ tx * 4 + 3 ] = ( col3 < _N ) ? _B[ bRow * _N + col3 ] : 0.0;
        }
        else
        {
            _Bs[ ty ][ tx * 4 + 0 ] = 0.0;
            _Bs[ ty ][ tx * 4 + 1 ] = 0.0;
            _Bs[ ty ][ tx * 4 + 2 ] = 0.0;
            _Bs[ ty ][ tx * 4 + 3 ] = 0.0;
        }

        GroupMemoryBarrierWithGroupSync();

        // --- Compute 4x2 outputs ---
        [unroll]
        for ( uint kk = 0; kk < TILE_K; kk++ )
        {
            float a0 = _As[ ty * 2 + 0 ][ kk ];
            float a1 = _As[ ty * 2 + 1 ][ kk ];

            float b0 = _Bs[ kk ][ tx * 4 + 0 ];
            float b1 = _Bs[ kk ][ tx * 4 + 1 ];
            float b2 = _Bs[ kk ][ tx * 4 + 2 ];
            float b3 = _Bs[ kk ][ tx * 4 + 3 ];

            c00 += a0 * b0;
            c01 += a0 * b1;
            c02 += a0 * b2;
            c03 += a0 * b3;

            c10 += a1 * b0;
            c11 += a1 * b1;
            c12 += a1 * b2;
            c13 += a1 * b3;
        }

        GroupMemoryBarrierWithGroupSync();
    }

    if ( row0 < _M )
    {
        if ( col0 < _N ) { uint idx = row0 * _N + col0; _Result[ idx ] = ( _Accumulate != 0 ) ? _Result[ idx ] + c00 : c00; }
        if ( col1 < _N ) { uint idx = row0 * _N + col1; _Result[ idx ] = ( _Accumulate != 0 ) ? _Result[ idx ] + c01 : c01; }
        if ( col2 < _N ) { uint idx = row0 * _N + col2; _Result[ idx ] = ( _Accumulate != 0 ) ? _Result[ idx ] + c02 : c02; }
        if ( col3 < _N ) { uint idx = row0 * _N + col3; _Result[ idx ] = ( _Accumulate != 0 ) ? _Result[ idx ] + c03 : c03; }
    }
    if ( row1 < _M )
    {
        if ( col0 < _N ) { uint idx = row1 * _N + col0; _Result[ idx ] = ( _Accumulate != 0 ) ? _Result[ idx ] + c10 : c10; }
        if ( col1 < _N ) { uint idx = row1 * _N + col1; _Result[ idx ] = ( _Accumulate != 0 ) ? _Result[ idx ] + c11 : c11; }
        if ( col2 < _N ) { uint idx = row1 * _N + col2; _Result[ idx ] = ( _Accumulate != 0 ) ? _Result[ idx ] + c12 : c12; }
        if ( col3 < _N ) { uint idx = row1 * _N + col3; _Result[ idx ] = ( _Accumulate != 0 ) ? _Result[ idx ] + c13 : c13; }
    }
}

//------------------------------------------------------------------
groupshared float _TransposeTile[ TILE ][ TILE + 1 ];

#pragma kernel Transpose
[numthreads( TILE, TILE, 1 )]
void Transpose( uint3 gid : SV_GroupID,
                uint3 tid : SV_GroupThreadID,
                uint3 dtid : SV_DispatchThreadID )
{
    // Input coordinates this thread loads
    uint inRow = dtid.y;
    uint inCol = dtid.x;

    // Load input tile into shared memory (contiguous reads)
    if ( inRow < _M && inCol < _N )
        _TransposeTile[ tid.y ][ tid.x ] = _Input[ inRow * _N + inCol ];

    GroupMemoryBarrierWithGroupSync();

    // Compute output coordinates for transposed write
    // Output is N x M
    uint outRow = gid.x * TILE + tid.y; // swapped
    uint outCol = gid.y * TILE + tid.x;

    if ( outRow < _N && outCol < _M )
        _Result[ outRow * _M + outCol ] = _TransposeTile[ tid.x ][ tid.y ];
}

// ============================================================
//  Data movement
// ============================================================

#pragma kernel Copy
[numthreads( THREADS, 1, 1 )]
void Copy( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Count ) return;
    _Dst[ _DstOffset + i ] = _Src[ _SrcOffset + i ];
}

//------------------------------------------------------------------
#pragma kernel SliceCopy
[numthreads( THREADS, 1, 1 )]
void SliceCopy( uint3 id : SV_DispatchThreadID )
{
    uint outer = id.x;
    if ( outer >= _OuterSize ) return;

    uint srcBase = outer * _SrcBlockSize + _StartOffset;
    uint dstBase = outer * _DstBlockSize;

    for ( uint i = 0; i < _InnerSize; i++ )
        _Dst[ dstBase + i ] = _Src[ srcBase + i ];
}

//------------------------------------------------------------------
#pragma kernel SliceCopyBackward
[numthreads( THREADS, 1, 1 )]
void SliceCopyBackward( uint3 id : SV_DispatchThreadID )
{
    uint outer = id.x;
    if ( outer >= _OuterSize ) return;

    uint srcBase = outer * _SrcBlockSize + _StartOffset;
    uint dstBase = outer * _DstBlockSize;

    for ( uint i = 0; i < _InnerSize; i++ )
        _Dst[ srcBase + i ] += _Src[ dstBase + i ];
}

//------------------------------------------------------------------
#pragma kernel ExpandLast
[numthreads( THREADS, 1, 1 )]
void ExpandLast( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float v = _Input[ i ];
    uint base_dst = i * _Num;
    for ( uint n = 0; n < _Num; n++ )
        _Result[ base_dst + n ] = v;
}

//------------------------------------------------------------------
#pragma kernel ExpandLastBackward
[numthreads( THREADS, 1, 1 )]
void ExpandLastBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float acc = 0.0;
    uint base_src = i * _Num;
    for ( uint n = 0; n < _Num; n++ )
        acc += _ResultGrad[ base_src + n ];
    _InputGrad[ i ] += acc;
}

//------------------------------------------------------------------
#pragma kernel Gather
[numthreads( THREADS, 1, 1 )]
void Gather( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Count ) return;

    uint srcRow = _Indices[ i ];
    uint srcStart = srcRow * _FeatureSize;
    uint dstStart = i * _FeatureSize;
    for ( uint f = 0; f < _FeatureSize; f++ )
        _Dst[ dstStart + f ] = _Src[ srcStart + f ];
}

// ============================================================
//  Utility
// ============================================================

#pragma kernel ZeroFill
[numthreads( THREADS, 1, 1 )]
void ZeroFill( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = 0.0;
}

//------------------------------------------------------------------
#pragma kernel OneFill
[numthreads( THREADS, 1, 1 )]
void OneFill( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = 1.0;
}

//------------------------------------------------------------------
#pragma kernel ValueFill
[numthreads( THREADS, 1, 1 )]
void ValueFill( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = _FillValue;
}

// ============================================================
//  Adam optimizer
// ============================================================

#pragma kernel AdamStep
[numthreads( THREADS, 1, 1 )]
void AdamStep( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;

    uint mi = _MomentOffset + i;
    float g = _Grad[ i ];

    float m = _Beta1 * _MBuf[ mi ] + ( 1.0 - _Beta1 ) * g;
    float v = _Beta2 * _VBuf[ mi ] + ( 1.0 - _Beta2 ) * g * g;

    _MBuf[ mi ] = m;
    _VBuf[ mi ] = v;

    float mHat = m * _InvBias1;
    float vHat = v * _InvBias2;

    _Data[ i ] -= _LR * mHat / ( sqrt( vHat ) + _Epsilon );
}
