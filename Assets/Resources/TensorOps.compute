// ============================================================
//  ChaosRL — Tensor Operations Compute Shader
//  All GPU kernels for ITensorBackend in a single file.
// ============================================================

#define THREADS 256

// ============================================================
//  Uniforms (set from GpuBackend before each dispatch)
// ============================================================

uint _Size;
uint _SizeA;
uint _SizeB;
uint _ResultSize;
float _Exponent;
float _MinVal;
float _MaxVal;
float _AGradScale;
float _BGradScale;

// MatMul / Transpose dimensions
uint _M;
uint _K;
uint _N;
uint _Accumulate;

// Reduction dimension decomposition
uint _OuterSize;
uint _DimSize;
uint _InnerSize;

// Slice / Copy
uint _SrcBlockSize;
uint _DstBlockSize;
uint _StartOffset;
uint _SrcOffset;
uint _DstOffset;
uint _Count;

// ExpandLast / Gather
uint _Num;
uint _FeatureSize;

// Adam optimizer
uint _MomentOffset;
float _LR;
float _Beta1;
float _Beta2;
float _Epsilon;
float _InvBias1;
float _InvBias2;

// Fill
float _FillValue;

// ============================================================
//  Buffer declarations
// ============================================================

// --- Forward binary / unary ---
StructuredBuffer<float>   _A;
StructuredBuffer<float>   _B;
RWStructuredBuffer<float> _Result;

// --- Unary input ---
StructuredBuffer<float>   _Input;

// --- Backward binary (uint for CAS-based atomic float add) ---
StructuredBuffer<float>   _AData;
StructuredBuffer<float>   _BData;
RWStructuredBuffer<uint>  _AGrad;
RWStructuredBuffer<uint>  _BGrad;
StructuredBuffer<float>   _ResultGrad;

// --- Backward unary ---
StructuredBuffer<float>   _InputData;
StructuredBuffer<float>   _ResultData;
RWStructuredBuffer<float> _InputGrad;

// --- Reduction output ---
RWStructuredBuffer<float> _Output;
RWStructuredBuffer<uint>  _MaxIdxBuf;

// --- Data movement ---
StructuredBuffer<float>   _Src;
RWStructuredBuffer<float> _Dst;

// --- Gather ---
StructuredBuffer<uint>    _Indices;

// --- Adam ---
RWStructuredBuffer<float> _Data;
StructuredBuffer<float>   _Grad;
RWStructuredBuffer<float> _MBuf;
RWStructuredBuffer<float> _VBuf;

// ============================================================
//  Float atomic add via CAS loop (for broadcast backward)
// ============================================================

void FloatAtomicAdd( RWStructuredBuffer<uint> buf, uint idx, float val )
{
    uint expected;
    uint original;
    [allow_uav_condition]
    do
    {
        expected = buf[ idx ];
        uint desired = asuint( asfloat( expected ) + val );
        InterlockedCompareExchange( buf[ idx ], expected, desired, original );
    }
    while ( original != expected );
}

// ============================================================
//  Element-wise binary — forward
// ============================================================

#pragma kernel ElementAdd
[numthreads( THREADS, 1, 1 )]
void ElementAdd( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    _Result[ i ] = _A[ i % _SizeA ] + _B[ i % _SizeB ];
}

//------------------------------------------------------------------
#pragma kernel ElementMul
[numthreads( THREADS, 1, 1 )]
void ElementMul( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    _Result[ i ] = _A[ i % _SizeA ] * _B[ i % _SizeB ];
}

//------------------------------------------------------------------
#pragma kernel ElementDiv
[numthreads( THREADS, 1, 1 )]
void ElementDiv( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    _Result[ i ] = _A[ i % _SizeA ] / _B[ i % _SizeB ];
}

//------------------------------------------------------------------
#pragma kernel ElementMax
[numthreads( THREADS, 1, 1 )]
void ElementMax( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float a = _A[ i % _SizeA ];
    float b = _B[ i % _SizeB ];
    _Result[ i ] = ( a >= b ) ? a : b;
}

//------------------------------------------------------------------
#pragma kernel ElementMin
[numthreads( THREADS, 1, 1 )]
void ElementMin( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float a = _A[ i % _SizeA ];
    float b = _B[ i % _SizeB ];
    _Result[ i ] = ( a <= b ) ? a : b;
}

// ============================================================
//  Element-wise binary — backward  (atomic float add)
// ============================================================

#pragma kernel AddBackward
[numthreads( THREADS, 1, 1 )]
void AddBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    if ( _AGradScale != 0.0 )
        FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg );
    if ( _BGradScale != 0.0 )
        FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * rg );
}

//------------------------------------------------------------------
#pragma kernel MulBackward
[numthreads( THREADS, 1, 1 )]
void MulBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( _AGradScale != 0.0 )
        FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * bd * rg );
    if ( _BGradScale != 0.0 )
        FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * ad * rg );
}

//------------------------------------------------------------------
#pragma kernel DivBackward
[numthreads( THREADS, 1, 1 )]
void DivBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( _AGradScale != 0.0 )
        FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg / bd );
    if ( _BGradScale != 0.0 )
        FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * ( -ad / ( bd * bd ) ) * rg );
}

//------------------------------------------------------------------
#pragma kernel ElementMaxBackward
[numthreads( THREADS, 1, 1 )]
void ElementMaxBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( ad >= bd )
    {
        if ( _AGradScale != 0.0 )
            FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg );
    }
    else
    {
        if ( _BGradScale != 0.0 )
            FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * rg );
    }
}

//------------------------------------------------------------------
#pragma kernel ElementMinBackward
[numthreads( THREADS, 1, 1 )]
void ElementMinBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( ad <= bd )
    {
        if ( _AGradScale != 0.0 )
            FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg );
    }
    else
    {
        if ( _BGradScale != 0.0 )
            FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * rg );
    }
}

// ============================================================
//  Element-wise unary — forward
// ============================================================

#pragma kernel Pow
[numthreads( THREADS, 1, 1 )]
void Pow( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = pow( abs( _Input[ i ] ), _Exponent ) * sign( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel Exp
[numthreads( THREADS, 1, 1 )]
void Exp( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = exp( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel Log
[numthreads( THREADS, 1, 1 )]
void Log( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = log( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel ReLU
[numthreads( THREADS, 1, 1 )]
void ReLU( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float v = _Input[ i ];
    _Result[ i ] = ( v > 0.0 ) ? v : 0.0;
}

//------------------------------------------------------------------
#pragma kernel Tanh
[numthreads( THREADS, 1, 1 )]
void Tanh( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = tanh( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel Clamp
[numthreads( THREADS, 1, 1 )]
void Clamp( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = clamp( _Input[ i ], _MinVal, _MaxVal );
}

// ============================================================
//  Element-wise unary — backward
// ============================================================

#pragma kernel PowBackward
[numthreads( THREADS, 1, 1 )]
void PowBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float x = _InputData[ i ];
    float rg = _ResultGrad[ i ];
    _InputGrad[ i ] += _Exponent * pow( abs( x ), _Exponent - 1.0 ) * sign( x ) * rg;
}

//------------------------------------------------------------------
#pragma kernel ExpBackward
[numthreads( THREADS, 1, 1 )]
void ExpBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _InputGrad[ i ] += _ResultData[ i ] * _ResultGrad[ i ];
}

//------------------------------------------------------------------
#pragma kernel LogBackward
[numthreads( THREADS, 1, 1 )]
void LogBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _InputGrad[ i ] += _ResultGrad[ i ] / _InputData[ i ];
}

//------------------------------------------------------------------
#pragma kernel ReLUBackward
[numthreads( THREADS, 1, 1 )]
void ReLUBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _InputGrad[ i ] += ( _InputData[ i ] > 0.0 ? 1.0 : 0.0 ) * _ResultGrad[ i ];
}

//------------------------------------------------------------------
#pragma kernel TanhBackward
[numthreads( THREADS, 1, 1 )]
void TanhBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float t = _ResultData[ i ];
    _InputGrad[ i ] += ( 1.0 - t * t ) * _ResultGrad[ i ];
}

//------------------------------------------------------------------
#pragma kernel ClampBackward
[numthreads( THREADS, 1, 1 )]
void ClampBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float x = _InputData[ i ];
    _InputGrad[ i ] += ( x >= _MinVal && x <= _MaxVal ? 1.0 : 0.0 ) * _ResultGrad[ i ];
}

// ============================================================
//  Reductions
// ============================================================

groupshared float _sharedF[ THREADS ];
groupshared uint  _sharedU[ THREADS ];

//------------------------------------------------------------------
#pragma kernel SumReduce
[numthreads( THREADS, 1, 1 )]
void SumReduce( uint3 dtid : SV_DispatchThreadID,
                uint3 gtid : SV_GroupThreadID,
                uint3 gid  : SV_GroupID )
{
    uint tid = gtid.x;
    _sharedF[ tid ] = ( dtid.x < _Size ) ? _Input[ dtid.x ] : 0.0;
    GroupMemoryBarrierWithGroupSync();

    for ( uint s = THREADS / 2; s > 0; s >>= 1 )
    {
        if ( tid < s )
            _sharedF[ tid ] += _sharedF[ tid + s ];
        GroupMemoryBarrierWithGroupSync();
    }

    if ( tid == 0 )
        _Output[ gid.x ] = _sharedF[ 0 ];
}

//------------------------------------------------------------------
#pragma kernel SumBackward
[numthreads( THREADS, 1, 1 )]
void SumBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    // _Output[0] holds the upstream scalar gradient; broadcast to all input grad slots
    _InputGrad[ i ] += _Output[ 0 ];
}

//------------------------------------------------------------------
#pragma kernel SumDim
[numthreads( THREADS, 1, 1 )]
void SumDim( uint3 id : SV_DispatchThreadID )
{
    // Each thread handles one (outer, inner) pair
    uint oi = id.x;
    if ( oi >= _OuterSize * _InnerSize ) return;

    uint outer = oi / _InnerSize;
    uint inner = oi % _InnerSize;

    float acc = 0.0;
    uint base_idx = outer * _DimSize * _InnerSize + inner;
    for ( uint d = 0; d < _DimSize; d++ )
        acc += _Input[ base_idx + d * _InnerSize ];

    _Output[ oi ] = acc;
}

//------------------------------------------------------------------
#pragma kernel SumDimBackward
[numthreads( THREADS, 1, 1 )]
void SumDimBackward( uint3 id : SV_DispatchThreadID )
{
    uint oi = id.x;
    if ( oi >= _OuterSize * _InnerSize ) return;

    uint outer = oi / _InnerSize;
    uint inner = oi % _InnerSize;

    float g = _ResultGrad[ oi ];
    uint base_idx = outer * _DimSize * _InnerSize + inner;
    for ( uint d = 0; d < _DimSize; d++ )
        _InputGrad[ base_idx + d * _InnerSize ] += g;
}

//------------------------------------------------------------------
#pragma kernel MaxReduce
[numthreads( THREADS, 1, 1 )]
void MaxReduce( uint3 dtid : SV_DispatchThreadID,
                uint3 gtid : SV_GroupThreadID,
                uint3 gid  : SV_GroupID )
{
    uint tid = gtid.x;
    if ( dtid.x < _Size )
    {
        _sharedF[ tid ] = _Input[ dtid.x ];
        _sharedU[ tid ] = dtid.x;
    }
    else
    {
        _sharedF[ tid ] = -3.402823e+38;
        _sharedU[ tid ] = 0;
    }
    GroupMemoryBarrierWithGroupSync();

    for ( uint s = THREADS / 2; s > 0; s >>= 1 )
    {
        if ( tid < s && _sharedF[ tid + s ] > _sharedF[ tid ] )
        {
            _sharedF[ tid ] = _sharedF[ tid + s ];
            _sharedU[ tid ] = _sharedU[ tid + s ];
        }
        GroupMemoryBarrierWithGroupSync();
    }

    if ( tid == 0 )
    {
        _Output[ gid.x ]     = _sharedF[ 0 ];
        _MaxIdxBuf[ gid.x ]  = _sharedU[ 0 ];
    }
}

//------------------------------------------------------------------
#pragma kernel MaxReduceBackward
[numthreads( 1, 1, 1 )]
void MaxReduceBackward( uint3 id : SV_DispatchThreadID )
{
    // Single-element: gradient flows only to the max position
    uint idx = _MaxIdxBuf[ 0 ];
    _InputGrad[ idx ] += _Output[ 0 ];
}

//------------------------------------------------------------------
#pragma kernel MaxReduceDim
[numthreads( THREADS, 1, 1 )]
void MaxReduceDim( uint3 id : SV_DispatchThreadID )
{
    uint oi = id.x;
    if ( oi >= _OuterSize * _InnerSize ) return;

    uint outer = oi / _InnerSize;
    uint inner = oi % _InnerSize;

    uint blockSize = _DimSize * _InnerSize;
    uint base_idx = outer * blockSize + inner;

    float maxVal = -3.402823e+38;
    uint  maxI   = 0;
    for ( uint d = 0; d < _DimSize; d++ )
    {
        uint idx = base_idx + d * _InnerSize;
        float v = _Input[ idx ];
        if ( v > maxVal )
        {
            maxVal = v;
            maxI   = idx;
        }
    }

    _Output[ oi ]     = maxVal;
    _MaxIdxBuf[ oi ]  = maxI;
}

//------------------------------------------------------------------
#pragma kernel MaxReduceDimBackward
[numthreads( THREADS, 1, 1 )]
void MaxReduceDimBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    uint srcIdx = _MaxIdxBuf[ i ];
    _InputGrad[ srcIdx ] += _ResultGrad[ i ];
}

// ============================================================
//  MatMul -- 64x64 output tile with 4x4 register micro-tiles
//  Thread group: 16x16 = 256 threads, each computes a 4x4 block
//  D3D11-safe register budget: acc[4][4]+regA[4]+regB[4]+overhead
//  = ~44 regs/thread x 256 = 11,264 < 16,384 limit
// ============================================================

#define BM  64
#define BN  64
#define BK  16
#define TM  4
#define TN  4

// Shared memory tiles (single declaration, reused by all MatMul variants)
// +1 padding avoids bank conflicts on column reads
groupshared float _smA[ BM ][ BK + 1 ];
groupshared float _smB[ BK ][ BN + 1 ];

#pragma kernel MatMulForward
[numthreads( BN / TN, BM / TM, 1 )]
void MatMulForward( uint3 gid  : SV_GroupID,
                    uint3 gtid : SV_GroupThreadID )
{
    uint threadCol = gtid.x;
    uint threadRow = gtid.y;
    uint flatIdx   = threadRow * ( BN / TN ) + threadCol;

    uint cRowBase = gid.y * BM;
    uint cColBase = gid.x * BN;

    uint microRow = threadRow * TM;
    uint microCol = threadCol * TN;

    // Declare loop counters once to avoid D3D11 HLSL scoping warnings
    uint mm_ri, mm_ci, mm_t, mm_ld, mm_k;

    float acc[ TM ][ TN ];
    [unroll] for ( mm_ri = 0; mm_ri < TM; mm_ri++ )
        [unroll] for ( mm_ci = 0; mm_ci < TN; mm_ci++ )
            acc[ mm_ri ][ mm_ci ] = 0.0;

    uint numTiles = ( _K + BK - 1 ) / BK;
    for ( mm_t = 0; mm_t < numTiles; mm_t++ )
    {
        uint kBase = mm_t * BK;

        // Load A tile: BM x BK = 1024 elements, 256 threads -> 4 per thread
        // Fast-varying thread index maps to K (column) -> coalesced A[row * K + col]
        [unroll] for ( mm_ld = 0; mm_ld < ( BM * BK / 256 ); mm_ld++ )
        {
            uint mm_li = flatIdx + mm_ld * 256;
            uint lRow  = mm_li / BK;
            uint lCol  = mm_li % BK;
            uint gRow  = cRowBase + lRow;
            uint gCol  = kBase + lCol;
            _smA[ lRow ][ lCol ] =
                ( gRow < _M && gCol < _K ) ? _A[ gRow * _K + gCol ] : 0.0;
        }

        // Load B tile: BK x BN = 1024 elements, 256 threads -> 4 per thread
        // Fast-varying thread index maps to N (column) -> coalesced B[row * N + col]
        [unroll] for ( mm_ld = 0; mm_ld < ( BK * BN / 256 ); mm_ld++ )
        {
            uint mm_li = flatIdx + mm_ld * 256;
            uint lRow  = mm_li / BN;
            uint lCol  = mm_li % BN;
            uint gRow  = kBase + lRow;
            uint gCol  = cColBase + lCol;
            _smB[ lRow ][ lCol ] =
                ( gRow < _K && gCol < _N ) ? _B[ gRow * _N + gCol ] : 0.0;
        }

        GroupMemoryBarrierWithGroupSync();

        // Outer product accumulation over K tile
        [unroll] for ( mm_k = 0; mm_k < BK; mm_k++ )
        {
            float regA[ TM ];
            float regB[ TN ];
            [unroll] for ( mm_ri = 0; mm_ri < TM; mm_ri++ )
                regA[ mm_ri ] = _smA[ microRow + mm_ri ][ mm_k ];
            [unroll] for ( mm_ci = 0; mm_ci < TN; mm_ci++ )
                regB[ mm_ci ] = _smB[ mm_k ][ microCol + mm_ci ];

            [unroll] for ( mm_ri = 0; mm_ri < TM; mm_ri++ )
                [unroll] for ( mm_ci = 0; mm_ci < TN; mm_ci++ )
                    acc[ mm_ri ][ mm_ci ] += regA[ mm_ri ] * regB[ mm_ci ];
        }

        GroupMemoryBarrierWithGroupSync();
    }

    // Write results
    [unroll] for ( mm_ri = 0; mm_ri < TM; mm_ri++ )
    {
        uint globalRow = cRowBase + microRow + mm_ri;
        if ( globalRow >= _M ) continue;
        [unroll] for ( mm_ci = 0; mm_ci < TN; mm_ci++ )
        {
            uint globalCol = cColBase + microCol + mm_ci;
            if ( globalCol >= _N ) continue;
            uint idx = globalRow * _N + globalCol;
            _Result[ idx ] = ( _Accumulate != 0 ) ? _Result[ idx ] + acc[ mm_ri ][ mm_ci ] : acc[ mm_ri ][ mm_ci ];
        }
    }
}

//------------------------------------------------------------------
// MatMulForwardBT -- C(M,N) = A(M,K) @ B^T, where B is stored as (N,K)
// B is read transposed on-the-fly: element (k, col) = B[ col * K + k ]
// Load pattern: fast-varying thread index maps to K for coalesced B access
//------------------------------------------------------------------

#pragma kernel MatMulForwardBT
[numthreads( BN / TN, BM / TM, 1 )]
void MatMulForwardBT( uint3 gid  : SV_GroupID,
                      uint3 gtid : SV_GroupThreadID )
{
    uint threadCol = gtid.x;
    uint threadRow = gtid.y;
    uint flatIdx   = threadRow * ( BN / TN ) + threadCol;

    uint cRowBase = gid.y * BM;
    uint cColBase = gid.x * BN;

    uint microRow = threadRow * TM;
    uint microCol = threadCol * TN;

    uint bt_ri, bt_ci, bt_t, bt_ld, bt_k;

    float acc[ TM ][ TN ];
    [unroll] for ( bt_ri = 0; bt_ri < TM; bt_ri++ )
        [unroll] for ( bt_ci = 0; bt_ci < TN; bt_ci++ )
            acc[ bt_ri ][ bt_ci ] = 0.0;

    uint numTiles = ( _K + BK - 1 ) / BK;
    for ( bt_t = 0; bt_t < numTiles; bt_t++ )
    {
        uint kBase = bt_t * BK;

        // Load A tile -- same as standard MatMul (coalesced along K)
        [unroll] for ( bt_ld = 0; bt_ld < ( BM * BK / 256 ); bt_ld++ )
        {
            uint li   = flatIdx + bt_ld * 256;
            uint lRow = li / BK;
            uint lCol = li % BK;
            uint gRow = cRowBase + lRow;
            uint gCol = kBase + lCol;
            _smA[ lRow ][ lCol ] =
                ( gRow < _M && gCol < _K ) ? _A[ gRow * _K + gCol ] : 0.0;
        }

        // Load B tile -- B stored as (N,K) row-major.
        // Fast-varying index maps to K dimension for coalesced B[n * K + k] access.
        // Destination: _smB[k_in_tile][n_in_tile]
        [unroll] for ( bt_ld = 0; bt_ld < ( BK * BN / 256 ); bt_ld++ )
        {
            uint li    = flatIdx + bt_ld * 256;
            uint smCol = li / BK;   // N-index within tile (slow varying)
            uint smRow = li % BK;   // K-index within tile (fast varying -> coalesced)
            uint gK    = kBase + smRow;
            uint gCol  = cColBase + smCol;
            _smB[ smRow ][ smCol ] =
                ( gK < _K && gCol < _N ) ? _B[ gCol * _K + gK ] : 0.0;
        }

        GroupMemoryBarrierWithGroupSync();

        [unroll] for ( bt_k = 0; bt_k < BK; bt_k++ )
        {
            float regA[ TM ];
            float regB[ TN ];
            [unroll] for ( bt_ri = 0; bt_ri < TM; bt_ri++ )
                regA[ bt_ri ] = _smA[ microRow + bt_ri ][ bt_k ];
            [unroll] for ( bt_ci = 0; bt_ci < TN; bt_ci++ )
                regB[ bt_ci ] = _smB[ bt_k ][ microCol + bt_ci ];

            [unroll] for ( bt_ri = 0; bt_ri < TM; bt_ri++ )
                [unroll] for ( bt_ci = 0; bt_ci < TN; bt_ci++ )
                    acc[ bt_ri ][ bt_ci ] += regA[ bt_ri ] * regB[ bt_ci ];
        }

        GroupMemoryBarrierWithGroupSync();
    }

    [unroll] for ( bt_ri = 0; bt_ri < TM; bt_ri++ )
    {
        uint globalRow = cRowBase + microRow + bt_ri;
        if ( globalRow >= _M ) continue;
        [unroll] for ( bt_ci = 0; bt_ci < TN; bt_ci++ )
        {
            uint globalCol = cColBase + microCol + bt_ci;
            if ( globalCol >= _N ) continue;
            uint idx = globalRow * _N + globalCol;
            _Result[ idx ] = ( _Accumulate != 0 ) ? _Result[ idx ] + acc[ bt_ri ][ bt_ci ] : acc[ bt_ri ][ bt_ci ];
        }
    }
}

//------------------------------------------------------------------
// MatMulForwardAT -- C(M,N) = A^T @ B, where A is stored as (K,M)
// A is read transposed on-the-fly: element (row, k) = A[ k * M + row ]
// Load pattern: fast-varying thread index maps to M for coalesced A access
//------------------------------------------------------------------

#pragma kernel MatMulForwardAT
[numthreads( BN / TN, BM / TM, 1 )]
void MatMulForwardAT( uint3 gid  : SV_GroupID,
                      uint3 gtid : SV_GroupThreadID )
{
    uint threadCol = gtid.x;
    uint threadRow = gtid.y;
    uint flatIdx   = threadRow * ( BN / TN ) + threadCol;

    uint cRowBase = gid.y * BM;
    uint cColBase = gid.x * BN;

    uint microRow = threadRow * TM;
    uint microCol = threadCol * TN;

    uint at_ri, at_ci, at_t, at_ld, at_k;

    float acc[ TM ][ TN ];
    [unroll] for ( at_ri = 0; at_ri < TM; at_ri++ )
        [unroll] for ( at_ci = 0; at_ci < TN; at_ci++ )
            acc[ at_ri ][ at_ci ] = 0.0;

    uint numTiles = ( _K + BK - 1 ) / BK;
    for ( at_t = 0; at_t < numTiles; at_t++ )
    {
        uint kBase = at_t * BK;

        // Load A tile -- A stored as (K,M) row-major.
        // Fast-varying index maps to M dimension for coalesced A[k * M + m] access.
        // Destination: _smA[m_in_tile][k_in_tile]
        [unroll] for ( at_ld = 0; at_ld < ( BM * BK / 256 ); at_ld++ )
        {
            uint li    = flatIdx + at_ld * 256;
            uint smCol = li / BM;   // K-index within tile (slow varying)
            uint smRow = li % BM;   // M-index within tile (fast varying -> coalesced)
            uint gRow  = cRowBase + smRow;
            uint gK    = kBase + smCol;
            _smA[ smRow ][ smCol ] =
                ( gRow < _M && gK < _K ) ? _A[ gK * _M + gRow ] : 0.0;
        }

        // Load B tile -- same as standard MatMul (coalesced along N)
        [unroll] for ( at_ld = 0; at_ld < ( BK * BN / 256 ); at_ld++ )
        {
            uint li   = flatIdx + at_ld * 256;
            uint lRow = li / BN;
            uint lCol = li % BN;
            uint gRow = kBase + lRow;
            uint gCol = cColBase + lCol;
            _smB[ lRow ][ lCol ] =
                ( gRow < _K && gCol < _N ) ? _B[ gRow * _N + gCol ] : 0.0;
        }

        GroupMemoryBarrierWithGroupSync();

        [unroll] for ( at_k = 0; at_k < BK; at_k++ )
        {
            float regA[ TM ];
            float regB[ TN ];
            [unroll] for ( at_ri = 0; at_ri < TM; at_ri++ )
                regA[ at_ri ] = _smA[ microRow + at_ri ][ at_k ];
            [unroll] for ( at_ci = 0; at_ci < TN; at_ci++ )
                regB[ at_ci ] = _smB[ at_k ][ microCol + at_ci ];

            [unroll] for ( at_ri = 0; at_ri < TM; at_ri++ )
                [unroll] for ( at_ci = 0; at_ci < TN; at_ci++ )
                    acc[ at_ri ][ at_ci ] += regA[ at_ri ] * regB[ at_ci ];
        }

        GroupMemoryBarrierWithGroupSync();
    }

    [unroll] for ( at_ri = 0; at_ri < TM; at_ri++ )
    {
        uint globalRow = cRowBase + microRow + at_ri;
        if ( globalRow >= _M ) continue;
        [unroll] for ( at_ci = 0; at_ci < TN; at_ci++ )
        {
            uint globalCol = cColBase + microCol + at_ci;
            if ( globalCol >= _N ) continue;
            uint idx = globalRow * _N + globalCol;
            _Result[ idx ] = ( _Accumulate != 0 ) ? _Result[ idx ] + acc[ at_ri ][ at_ci ] : acc[ at_ri ][ at_ci ];
        }
    }
}

//------------------------------------------------------------------
//  Tiled transpose — 32x32 tiles with shared memory
//  Coalesced reads and writes, +1 padding to avoid bank conflicts
//------------------------------------------------------------------

#define TRANS_TILE 32

groupshared float _transTile[ TRANS_TILE ][ TRANS_TILE + 1 ];

#pragma kernel Transpose
[numthreads( TRANS_TILE, 8, 1 )]
void Transpose( uint3 gid : SV_GroupID,
                uint3 tid : SV_GroupThreadID )
{
    uint xBlock = gid.x * TRANS_TILE;
    uint yBlock = gid.y * TRANS_TILE;

    // Declare loop counter once to avoid D3D11 HLSL scoping warnings
    uint tr_i;

    // Coalesced load -- each of 256 threads covers 4 rows
    [unroll] for ( tr_i = 0; tr_i < 4; tr_i++ )
    {
        uint row = yBlock + tid.y + tr_i * 8;
        uint col = xBlock + tid.x;
        _transTile[ tid.y + tr_i * 8 ][ tid.x ] =
            ( row < _M && col < _N ) ? _Input[ row * _N + col ] : 0.0;
    }

    GroupMemoryBarrierWithGroupSync();

    // Coalesced write -- transposed block position
    [unroll] for ( tr_i = 0; tr_i < 4; tr_i++ )
    {
        uint outRow = xBlock + tid.y + tr_i * 8;
        uint outCol = yBlock + tid.x;
        if ( outRow < _N && outCol < _M )
            _Result[ outRow * _M + outCol ] = _transTile[ tid.x ][ tid.y + tr_i * 8 ];
    }
}

// ============================================================
//  Data movement
// ============================================================

#pragma kernel Copy
[numthreads( THREADS, 1, 1 )]
void Copy( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Count ) return;
    _Dst[ _DstOffset + i ] = _Src[ _SrcOffset + i ];
}

//------------------------------------------------------------------
#pragma kernel SliceCopy
[numthreads( THREADS, 1, 1 )]
void SliceCopy( uint3 id : SV_DispatchThreadID )
{
    uint outer = id.x;
    if ( outer >= _OuterSize ) return;

    uint srcBase = outer * _SrcBlockSize + _StartOffset;
    uint dstBase = outer * _DstBlockSize;

    for ( uint i = 0; i < _InnerSize; i++ )
        _Dst[ dstBase + i ] = _Src[ srcBase + i ];
}

//------------------------------------------------------------------
#pragma kernel SliceCopyBackward
[numthreads( THREADS, 1, 1 )]
void SliceCopyBackward( uint3 id : SV_DispatchThreadID )
{
    uint outer = id.x;
    if ( outer >= _OuterSize ) return;

    uint srcBase = outer * _SrcBlockSize + _StartOffset;
    uint dstBase = outer * _DstBlockSize;

    for ( uint i = 0; i < _InnerSize; i++ )
        _Dst[ srcBase + i ] += _Src[ dstBase + i ];
}

//------------------------------------------------------------------
#pragma kernel ExpandLast
[numthreads( THREADS, 1, 1 )]
void ExpandLast( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float v = _Input[ i ];
    uint base_dst = i * _Num;
    for ( uint n = 0; n < _Num; n++ )
        _Result[ base_dst + n ] = v;
}

//------------------------------------------------------------------
#pragma kernel ExpandLastBackward
[numthreads( THREADS, 1, 1 )]
void ExpandLastBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float acc = 0.0;
    uint base_src = i * _Num;
    for ( uint n = 0; n < _Num; n++ )
        acc += _ResultGrad[ base_src + n ];
    _InputGrad[ i ] += acc;
}

//------------------------------------------------------------------
#pragma kernel Gather
[numthreads( THREADS, 1, 1 )]
void Gather( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Count ) return;

    uint srcRow = _Indices[ i ];
    uint srcStart = srcRow * _FeatureSize;
    uint dstStart = i * _FeatureSize;
    for ( uint f = 0; f < _FeatureSize; f++ )
        _Dst[ dstStart + f ] = _Src[ srcStart + f ];
}

// ============================================================
//  Utility
// ============================================================

#pragma kernel ZeroFill
[numthreads( THREADS, 1, 1 )]
void ZeroFill( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = 0.0;
}

//------------------------------------------------------------------
#pragma kernel OneFill
[numthreads( THREADS, 1, 1 )]
void OneFill( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = 1.0;
}

//------------------------------------------------------------------
#pragma kernel ValueFill
[numthreads( THREADS, 1, 1 )]
void ValueFill( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = _FillValue;
}

// ============================================================
//  Adam optimizer
// ============================================================

#pragma kernel AdamStep
[numthreads( THREADS, 1, 1 )]
void AdamStep( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;

    uint mi = _MomentOffset + i;
    float g = _Grad[ i ];

    float m = _Beta1 * _MBuf[ mi ] + ( 1.0 - _Beta1 ) * g;
    float v = _Beta2 * _VBuf[ mi ] + ( 1.0 - _Beta2 ) * g * g;

    _MBuf[ mi ] = m;
    _VBuf[ mi ] = v;

    float mHat = m * _InvBias1;
    float vHat = v * _InvBias2;

    _Data[ i ] -= _LR * mHat / ( sqrt( vHat ) + _Epsilon );
}
