// ============================================================
//  ChaosRL — Tensor Operations Compute Shader
//  All GPU kernels for ITensorBackend in a single file.
// ============================================================

#define THREADS 256

// ============================================================
//  Uniforms (set from GpuBackend before each dispatch)
// ============================================================

uint _Size;
uint _SizeA;
uint _SizeB;
uint _ResultSize;
float _Exponent;
float _MinVal;
float _MaxVal;
float _AGradScale;
float _BGradScale;

// MatMul / Transpose dimensions
uint _M;
uint _K;
uint _N;
uint _Accumulate;

// Reduction dimension decomposition
uint _OuterSize;
uint _DimSize;
uint _InnerSize;

// Slice / Copy
uint _SrcBlockSize;
uint _DstBlockSize;
uint _StartOffset;
uint _SrcOffset;
uint _DstOffset;
uint _Count;

// ExpandLast / Gather
uint _Num;
uint _FeatureSize;

// Adam optimizer
uint _MomentOffset;
float _LR;
float _Beta1;
float _Beta2;
float _Epsilon;
float _InvBias1;
float _InvBias2;

// Fill
float _FillValue;

// ============================================================
//  Buffer declarations
// ============================================================

// --- Forward binary / unary ---
StructuredBuffer<float>   _A;
StructuredBuffer<float>   _B;
RWStructuredBuffer<float> _Result;

// --- Unary input ---
StructuredBuffer<float>   _Input;

// --- Backward binary (uint for CAS-based atomic float add) ---
StructuredBuffer<float>   _AData;
StructuredBuffer<float>   _BData;
RWStructuredBuffer<uint>  _AGrad;
RWStructuredBuffer<uint>  _BGrad;
StructuredBuffer<float>   _ResultGrad;

// --- Backward unary ---
StructuredBuffer<float>   _InputData;
StructuredBuffer<float>   _ResultData;
RWStructuredBuffer<float> _InputGrad;

// --- Reduction output ---
RWStructuredBuffer<float> _Output;
RWStructuredBuffer<uint>  _MaxIdxBuf;

// --- Data movement ---
StructuredBuffer<float>   _Src;
RWStructuredBuffer<float> _Dst;

// --- Gather ---
StructuredBuffer<uint>    _Indices;

// --- Adam ---
RWStructuredBuffer<float> _Data;
StructuredBuffer<float>   _Grad;
RWStructuredBuffer<float> _MBuf;
RWStructuredBuffer<float> _VBuf;

// ============================================================
//  Float atomic add via CAS loop (for broadcast backward)
// ============================================================

void FloatAtomicAdd( RWStructuredBuffer<uint> buf, uint idx, float val )
{
    uint expected;
    uint original;
    [allow_uav_condition]
    do
    {
        expected = buf[ idx ];
        uint desired = asuint( asfloat( expected ) + val );
        InterlockedCompareExchange( buf[ idx ], expected, desired, original );
    }
    while ( original != expected );
}

// ============================================================
//  Element-wise binary — forward
// ============================================================

#pragma kernel ElementAdd
[numthreads( THREADS, 1, 1 )]
void ElementAdd( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    _Result[ i ] = _A[ i % _SizeA ] + _B[ i % _SizeB ];
}

//------------------------------------------------------------------
#pragma kernel ElementMul
[numthreads( THREADS, 1, 1 )]
void ElementMul( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    _Result[ i ] = _A[ i % _SizeA ] * _B[ i % _SizeB ];
}

//------------------------------------------------------------------
#pragma kernel ElementDiv
[numthreads( THREADS, 1, 1 )]
void ElementDiv( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    _Result[ i ] = _A[ i % _SizeA ] / _B[ i % _SizeB ];
}

//------------------------------------------------------------------
#pragma kernel ElementMax
[numthreads( THREADS, 1, 1 )]
void ElementMax( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float a = _A[ i % _SizeA ];
    float b = _B[ i % _SizeB ];
    _Result[ i ] = ( a >= b ) ? a : b;
}

//------------------------------------------------------------------
#pragma kernel ElementMin
[numthreads( THREADS, 1, 1 )]
void ElementMin( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float a = _A[ i % _SizeA ];
    float b = _B[ i % _SizeB ];
    _Result[ i ] = ( a <= b ) ? a : b;
}

// ============================================================
//  Element-wise binary — backward  (atomic float add)
// ============================================================

#pragma kernel AddBackward
[numthreads( THREADS, 1, 1 )]
void AddBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    if ( _AGradScale != 0.0 )
        FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg );
    if ( _BGradScale != 0.0 )
        FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * rg );
}

//------------------------------------------------------------------
#pragma kernel MulBackward
[numthreads( THREADS, 1, 1 )]
void MulBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( _AGradScale != 0.0 )
        FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * bd * rg );
    if ( _BGradScale != 0.0 )
        FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * ad * rg );
}

//------------------------------------------------------------------
#pragma kernel DivBackward
[numthreads( THREADS, 1, 1 )]
void DivBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( _AGradScale != 0.0 )
        FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg / bd );
    if ( _BGradScale != 0.0 )
        FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * ( -ad / ( bd * bd ) ) * rg );
}

//------------------------------------------------------------------
#pragma kernel ElementMaxBackward
[numthreads( THREADS, 1, 1 )]
void ElementMaxBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( ad >= bd )
    {
        if ( _AGradScale != 0.0 )
            FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg );
    }
    else
    {
        if ( _BGradScale != 0.0 )
            FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * rg );
    }
}

//------------------------------------------------------------------
#pragma kernel ElementMinBackward
[numthreads( THREADS, 1, 1 )]
void ElementMinBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _ResultSize ) return;
    float rg = _ResultGrad[ i ];
    float ad = _AData[ i % _SizeA ];
    float bd = _BData[ i % _SizeB ];
    if ( ad <= bd )
    {
        if ( _AGradScale != 0.0 )
            FloatAtomicAdd( _AGrad, i % _SizeA, _AGradScale * rg );
    }
    else
    {
        if ( _BGradScale != 0.0 )
            FloatAtomicAdd( _BGrad, i % _SizeB, _BGradScale * rg );
    }
}

// ============================================================
//  Element-wise unary — forward
// ============================================================

#pragma kernel Pow
[numthreads( THREADS, 1, 1 )]
void Pow( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = pow( abs( _Input[ i ] ), _Exponent ) * sign( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel Exp
[numthreads( THREADS, 1, 1 )]
void Exp( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = exp( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel Log
[numthreads( THREADS, 1, 1 )]
void Log( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = log( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel ReLU
[numthreads( THREADS, 1, 1 )]
void ReLU( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float v = _Input[ i ];
    _Result[ i ] = ( v > 0.0 ) ? v : 0.0;
}

//------------------------------------------------------------------
#pragma kernel Tanh
[numthreads( THREADS, 1, 1 )]
void Tanh( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = tanh( _Input[ i ] );
}

//------------------------------------------------------------------
#pragma kernel Clamp
[numthreads( THREADS, 1, 1 )]
void Clamp( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = clamp( _Input[ i ], _MinVal, _MaxVal );
}

// ============================================================
//  Element-wise unary — backward
// ============================================================

#pragma kernel PowBackward
[numthreads( THREADS, 1, 1 )]
void PowBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float x = _InputData[ i ];
    float rg = _ResultGrad[ i ];
    _InputGrad[ i ] += _Exponent * pow( abs( x ), _Exponent - 1.0 ) * sign( x ) * rg;
}

//------------------------------------------------------------------
#pragma kernel ExpBackward
[numthreads( THREADS, 1, 1 )]
void ExpBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _InputGrad[ i ] += _ResultData[ i ] * _ResultGrad[ i ];
}

//------------------------------------------------------------------
#pragma kernel LogBackward
[numthreads( THREADS, 1, 1 )]
void LogBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _InputGrad[ i ] += _ResultGrad[ i ] / _InputData[ i ];
}

//------------------------------------------------------------------
#pragma kernel ReLUBackward
[numthreads( THREADS, 1, 1 )]
void ReLUBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _InputGrad[ i ] += ( _InputData[ i ] > 0.0 ? 1.0 : 0.0 ) * _ResultGrad[ i ];
}

//------------------------------------------------------------------
#pragma kernel TanhBackward
[numthreads( THREADS, 1, 1 )]
void TanhBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float t = _ResultData[ i ];
    _InputGrad[ i ] += ( 1.0 - t * t ) * _ResultGrad[ i ];
}

//------------------------------------------------------------------
#pragma kernel ClampBackward
[numthreads( THREADS, 1, 1 )]
void ClampBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float x = _InputData[ i ];
    _InputGrad[ i ] += ( x >= _MinVal && x <= _MaxVal ? 1.0 : 0.0 ) * _ResultGrad[ i ];
}

// ============================================================
//  Reductions
// ============================================================

groupshared float _sharedF[ THREADS ];
groupshared uint  _sharedU[ THREADS ];

//------------------------------------------------------------------
#pragma kernel SumReduce
[numthreads( THREADS, 1, 1 )]
void SumReduce( uint3 dtid : SV_DispatchThreadID,
                uint3 gtid : SV_GroupThreadID,
                uint3 gid  : SV_GroupID )
{
    uint tid = gtid.x;
    _sharedF[ tid ] = ( dtid.x < _Size ) ? _Input[ dtid.x ] : 0.0;
    GroupMemoryBarrierWithGroupSync();

    for ( uint s = THREADS / 2; s > 0; s >>= 1 )
    {
        if ( tid < s )
            _sharedF[ tid ] += _sharedF[ tid + s ];
        GroupMemoryBarrierWithGroupSync();
    }

    if ( tid == 0 )
        _Output[ gid.x ] = _sharedF[ 0 ];
}

//------------------------------------------------------------------
#pragma kernel SumBackward
[numthreads( THREADS, 1, 1 )]
void SumBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    // _Output[0] holds the upstream scalar gradient; broadcast to all input grad slots
    _InputGrad[ i ] += _Output[ 0 ];
}

//------------------------------------------------------------------
#pragma kernel SumDim
[numthreads( THREADS, 1, 1 )]
void SumDim( uint3 id : SV_DispatchThreadID )
{
    // Each thread handles one (outer, inner) pair
    uint oi = id.x;
    if ( oi >= _OuterSize * _InnerSize ) return;

    uint outer = oi / _InnerSize;
    uint inner = oi % _InnerSize;

    float acc = 0.0;
    uint base_idx = outer * _DimSize * _InnerSize + inner;
    for ( uint d = 0; d < _DimSize; d++ )
        acc += _Input[ base_idx + d * _InnerSize ];

    _Output[ oi ] = acc;
}

//------------------------------------------------------------------
#pragma kernel SumDimBackward
[numthreads( THREADS, 1, 1 )]
void SumDimBackward( uint3 id : SV_DispatchThreadID )
{
    uint oi = id.x;
    if ( oi >= _OuterSize * _InnerSize ) return;

    uint outer = oi / _InnerSize;
    uint inner = oi % _InnerSize;

    float g = _ResultGrad[ oi ];
    uint base_idx = outer * _DimSize * _InnerSize + inner;
    for ( uint d = 0; d < _DimSize; d++ )
        _InputGrad[ base_idx + d * _InnerSize ] += g;
}

//------------------------------------------------------------------
#pragma kernel MaxReduce
[numthreads( THREADS, 1, 1 )]
void MaxReduce( uint3 dtid : SV_DispatchThreadID,
                uint3 gtid : SV_GroupThreadID,
                uint3 gid  : SV_GroupID )
{
    uint tid = gtid.x;
    if ( dtid.x < _Size )
    {
        _sharedF[ tid ] = _Input[ dtid.x ];
        _sharedU[ tid ] = dtid.x;
    }
    else
    {
        _sharedF[ tid ] = -3.402823e+38;
        _sharedU[ tid ] = 0;
    }
    GroupMemoryBarrierWithGroupSync();

    for ( uint s = THREADS / 2; s > 0; s >>= 1 )
    {
        if ( tid < s && _sharedF[ tid + s ] > _sharedF[ tid ] )
        {
            _sharedF[ tid ] = _sharedF[ tid + s ];
            _sharedU[ tid ] = _sharedU[ tid + s ];
        }
        GroupMemoryBarrierWithGroupSync();
    }

    if ( tid == 0 )
    {
        _Output[ gid.x ]     = _sharedF[ 0 ];
        _MaxIdxBuf[ gid.x ]  = _sharedU[ 0 ];
    }
}

//------------------------------------------------------------------
#pragma kernel MaxReduceBackward
[numthreads( 1, 1, 1 )]
void MaxReduceBackward( uint3 id : SV_DispatchThreadID )
{
    // Single-element: gradient flows only to the max position
    uint idx = _MaxIdxBuf[ 0 ];
    _InputGrad[ idx ] += _Output[ 0 ];
}

//------------------------------------------------------------------
#pragma kernel MaxReduceDim
[numthreads( THREADS, 1, 1 )]
void MaxReduceDim( uint3 id : SV_DispatchThreadID )
{
    uint oi = id.x;
    if ( oi >= _OuterSize * _InnerSize ) return;

    uint outer = oi / _InnerSize;
    uint inner = oi % _InnerSize;

    uint blockSize = _DimSize * _InnerSize;
    uint base_idx = outer * blockSize + inner;

    float maxVal = -3.402823e+38;
    uint  maxI   = 0;
    for ( uint d = 0; d < _DimSize; d++ )
    {
        uint idx = base_idx + d * _InnerSize;
        float v = _Input[ idx ];
        if ( v > maxVal )
        {
            maxVal = v;
            maxI   = idx;
        }
    }

    _Output[ oi ]     = maxVal;
    _MaxIdxBuf[ oi ]  = maxI;
}

//------------------------------------------------------------------
#pragma kernel MaxReduceDimBackward
[numthreads( THREADS, 1, 1 )]
void MaxReduceDimBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    uint srcIdx = _MaxIdxBuf[ i ];
    _InputGrad[ srcIdx ] += _ResultGrad[ i ];
}

// ============================================================
//  MatMul -- 128x128 symmetric output tile, 8x4 micro-tiles
//  Thread group: 16x16 = 256 threads, each computes an 8x4 block
//  Register prefetch: loads next tile into registers while computing current
//  tile, allowing the GPU scheduler to overlap memory latency with FMA.
//  Scalar accumulators avoid D3D11 "indexable temp" register penalty.
// ============================================================

#define BM  128
#define BN  64
#define BK  16
#define TM  8
#define TN  4
#define THREADS_PER_GROUP ((BM/TM)*(BN/TN))
#define N_A_LOADS (BM * BK / THREADS_PER_GROUP)
#define N_B_LOADS (BK * BN / THREADS_PER_GROUP)

// Shared memory tiles (single declaration, reused by all MatMul variants)
// +1 padding avoids bank conflicts on column reads
groupshared float _smA[ BM ][ BK + 1 ];
groupshared float _smB[ BK ][ BN + 1 ];

// ------------------------------------------------------------------
//  Macro: scalar 8x4 outer-product accumulation from shared memory.
//  Uses 32 scalar accumulators (c00..c73) to avoid indexable temps.
//  Expects: microRow, microCol, mm_k defined in caller scope.
// ------------------------------------------------------------------
#define OUTER_PRODUCT_8x4                                       \
{                                                               \
    float a0 = _smA[ microRow     ][ mm_k ];                    \
    float a1 = _smA[ microRow + 1 ][ mm_k ];                    \
    float a2 = _smA[ microRow + 2 ][ mm_k ];                    \
    float a3 = _smA[ microRow + 3 ][ mm_k ];                    \
    float a4 = _smA[ microRow + 4 ][ mm_k ];                    \
    float a5 = _smA[ microRow + 5 ][ mm_k ];                    \
    float a6 = _smA[ microRow + 6 ][ mm_k ];                    \
    float a7 = _smA[ microRow + 7 ][ mm_k ];                    \
    float b0 = _smB[ mm_k ][ microCol     ];                    \
    float b1 = _smB[ mm_k ][ microCol + 1 ];                    \
    float b2 = _smB[ mm_k ][ microCol + 2 ];                    \
    float b3 = _smB[ mm_k ][ microCol + 3 ];                    \
    c00 += a0*b0; c01 += a0*b1; c02 += a0*b2; c03 += a0*b3;    \
    c10 += a1*b0; c11 += a1*b1; c12 += a1*b2; c13 += a1*b3;    \
    c20 += a2*b0; c21 += a2*b1; c22 += a2*b2; c23 += a2*b3;    \
    c30 += a3*b0; c31 += a3*b1; c32 += a3*b2; c33 += a3*b3;    \
    c40 += a4*b0; c41 += a4*b1; c42 += a4*b2; c43 += a4*b3;    \
    c50 += a5*b0; c51 += a5*b1; c52 += a5*b2; c53 += a5*b3;    \
    c60 += a6*b0; c61 += a6*b1; c62 += a6*b2; c63 += a6*b3;    \
    c70 += a7*b0; c71 += a7*b1; c72 += a7*b2; c73 += a7*b3;    \
}

// ------------------------------------------------------------------
//  Macro: write-back 8x4 scalar accumulators to global memory.
//  Expects: cRowBase, microRow, cColBase, microCol, _Accumulate.
// ------------------------------------------------------------------
#define WB_ELEM( R, C, ACC )                                                                            \
    {                                                                                                   \
        uint wgc = cColBase + microCol + (C);                                                           \
        if ( wgc < _N ) { uint wi = (cRowBase + microRow + (R)) * _N + wgc;                             \
            _Result[wi] = (_Accumulate != 0) ? _Result[wi] + ACC : ACC; }                               \
    }

#define WB_ROW( R, C0, C1, C2, C3 )                                                                    \
    if ( cRowBase + microRow + (R) < _M ) { WB_ELEM(R,0,C0) WB_ELEM(R,1,C1) WB_ELEM(R,2,C2) WB_ELEM(R,3,C3) }

#define WRITEBACK_8x4                                           \
{                                                               \
    WB_ROW( 0, c00, c01, c02, c03 )                             \
    WB_ROW( 1, c10, c11, c12, c13 )                             \
    WB_ROW( 2, c20, c21, c22, c23 )                             \
    WB_ROW( 3, c30, c31, c32, c33 )                             \
    WB_ROW( 4, c40, c41, c42, c43 )                             \
    WB_ROW( 5, c50, c51, c52, c53 )                             \
    WB_ROW( 6, c60, c61, c62, c63 )                             \
    WB_ROW( 7, c70, c71, c72, c73 )                             \
}

// ------------------------------------------------------------------
//  Macro: declare + zero-init the 32 scalar accumulators.
// ------------------------------------------------------------------
#define DECLARE_ACC_8x4                                         \
    float c00=0,c01=0,c02=0,c03=0;                             \
    float c10=0,c11=0,c12=0,c13=0;                             \
    float c20=0,c21=0,c22=0,c23=0;                             \
    float c30=0,c31=0,c32=0,c33=0;                             \
    float c40=0,c41=0,c42=0,c43=0;                             \
    float c50=0,c51=0,c52=0,c53=0;                             \
    float c60=0,c61=0,c62=0,c63=0;                             \
    float c70=0,c71=0,c72=0,c73=0;

// ------------------------------------------------------------------
//  Register prefetch: 12 scalar registers to hold the next tile's
//  data while computing on the current shared-memory tile.
//  A tile: N_A_LOADS = 8 elements/thread, B tile: N_B_LOADS = 4.
// ------------------------------------------------------------------
#define DECLARE_PREFETCH                                        \
    float rA0, rA1, rA2, rA3, rA4, rA5, rA6, rA7;             \
    float rB0, rB1, rB2, rB3;

// --- Element-level prefetch: load one element from global to register ---

#define PF_ASTD( REG, LD, KB )                                  \
{                                                               \
    uint _i = flatIdx + (LD) * THREADS_PER_GROUP;               \
    uint _r = cRowBase + _i / BK;                               \
    uint _c = (KB) + _i % BK;                                   \
    REG = ( _r < _M && _c < _K ) ? _A[ _r * _K + _c ] : 0.0;  \
}

#define PF_BSTD( REG, LD, KB )                                  \
{                                                               \
    uint _i = flatIdx + (LD) * THREADS_PER_GROUP;               \
    uint _r = (KB) + _i / BN;                                   \
    uint _c = cColBase + _i % BN;                               \
    REG = ( _r < _K && _c < _N ) ? _B[ _r * _N + _c ] : 0.0;  \
}

#define PF_BBT( REG, LD, KB )                                   \
{                                                               \
    uint _i = flatIdx + (LD) * THREADS_PER_GROUP;               \
    uint _gK = (KB) + _i % BK;                                  \
    uint _gC = cColBase + _i / BK;                              \
    REG = ( _gK < _K && _gC < _N ) ? _B[ _gC * _K + _gK ] : 0.0; \
}

#define PF_AAT( REG, LD, KB )                                   \
{                                                               \
    uint _i = flatIdx + (LD) * THREADS_PER_GROUP;               \
    uint _gR = cRowBase + _i % BM;                              \
    uint _gK = (KB) + _i / BM;                                  \
    REG = ( _gR < _M && _gK < _K ) ? _A[ _gK * _M + _gR ] : 0.0; \
}

// --- Element-level store: write one register to shared memory ---

#define ST_ASTD( REG, LD )                                      \
{                                                               \
    uint _i = flatIdx + (LD) * THREADS_PER_GROUP;               \
    _smA[ _i / BK ][ _i % BK ] = REG;                          \
}

#define ST_BSTD( REG, LD )                                      \
{                                                               \
    uint _i = flatIdx + (LD) * THREADS_PER_GROUP;               \
    _smB[ _i / BN ][ _i % BN ] = REG;                          \
}

#define ST_BBT( REG, LD )                                       \
{                                                               \
    uint _i = flatIdx + (LD) * THREADS_PER_GROUP;               \
    _smB[ _i % BK ][ _i / BK ] = REG;                          \
}

#define ST_AAT( REG, LD )                                       \
{                                                               \
    uint _i = flatIdx + (LD) * THREADS_PER_GROUP;               \
    _smA[ _i % BM ][ _i / BM ] = REG;                          \
}

// --- Aggregate prefetch: load full tile to registers ---

#define PF_A_STD_ALL( KB )                                                                          \
    PF_ASTD(rA0,0,KB) PF_ASTD(rA1,1,KB) PF_ASTD(rA2,2,KB) PF_ASTD(rA3,3,KB)                      \
    PF_ASTD(rA4,4,KB) PF_ASTD(rA5,5,KB) PF_ASTD(rA6,6,KB) PF_ASTD(rA7,7,KB)

#define PF_B_STD_ALL( KB )                                                                          \
    PF_BSTD(rB0,0,KB) PF_BSTD(rB1,1,KB) PF_BSTD(rB2,2,KB) PF_BSTD(rB3,3,KB)

#define PF_B_BT_ALL( KB )                                                                           \
    PF_BBT(rB0,0,KB) PF_BBT(rB1,1,KB) PF_BBT(rB2,2,KB) PF_BBT(rB3,3,KB)

#define PF_A_AT_ALL( KB )                                                                            \
    PF_AAT(rA0,0,KB) PF_AAT(rA1,1,KB) PF_AAT(rA2,2,KB) PF_AAT(rA3,3,KB)                          \
    PF_AAT(rA4,4,KB) PF_AAT(rA5,5,KB) PF_AAT(rA6,6,KB) PF_AAT(rA7,7,KB)

// --- Aggregate store: write all registers to shared ---

#define ST_A_STD_ALL                                                                                 \
    ST_ASTD(rA0,0) ST_ASTD(rA1,1) ST_ASTD(rA2,2) ST_ASTD(rA3,3)                                   \
    ST_ASTD(rA4,4) ST_ASTD(rA5,5) ST_ASTD(rA6,6) ST_ASTD(rA7,7)

#define ST_B_STD_ALL                                                                                 \
    ST_BSTD(rB0,0) ST_BSTD(rB1,1) ST_BSTD(rB2,2) ST_BSTD(rB3,3)

#define ST_B_BT_ALL                                                                                  \
    ST_BBT(rB0,0) ST_BBT(rB1,1) ST_BBT(rB2,2) ST_BBT(rB3,3)

#define ST_A_AT_ALL                                                                                  \
    ST_AAT(rA0,0) ST_AAT(rA1,1) ST_AAT(rA2,2) ST_AAT(rA3,3)                                       \
    ST_AAT(rA4,4) ST_AAT(rA5,5) ST_AAT(rA6,6) ST_AAT(rA7,7)

// ==================================================================
//  MatMulForward -- C(M,N) = A(M,K) @ B(K,N)
//  Double-buffered: prefetch next tile to registers while computing
//  current tile from shared memory.
// ==================================================================

#pragma kernel MatMulForward
[numthreads( BN / TN, BM / TM, 1 )]
void MatMulForward( uint3 gid  : SV_GroupID,
                    uint3 gtid : SV_GroupThreadID )
{
    uint threadCol = gtid.x;
    uint threadRow = gtid.y;
    uint flatIdx   = threadRow * ( BN / TN ) + threadCol;

    uint cRowBase = gid.y * BM;
    uint cColBase = gid.x * BN;

    uint microRow = threadRow * TM;
    uint microCol = threadCol * TN;

    uint mm_t, mm_k, kBase;

    DECLARE_ACC_8x4
    DECLARE_PREFETCH

    uint numTiles = ( _K + BK - 1 ) / BK;

    // --- Prologue: load first tile directly to shared ---
    PF_A_STD_ALL( 0 )
    PF_B_STD_ALL( 0 )
    ST_A_STD_ALL
    ST_B_STD_ALL
    GroupMemoryBarrierWithGroupSync();

    // --- Main loop: prefetch next tile while computing current ---
    for ( mm_t = 1; mm_t < numTiles; mm_t++ )
    {
        kBase = mm_t * BK;

        // Issue global loads into registers (will overlap with FMAs below)
        PF_A_STD_ALL( kBase )
        PF_B_STD_ALL( kBase )

        // Compute on current shared memory tile
        [unroll] for ( mm_k = 0; mm_k < BK; mm_k++ )
            OUTER_PRODUCT_8x4

        GroupMemoryBarrierWithGroupSync();

        // Write prefetched registers to shared memory
        ST_A_STD_ALL
        ST_B_STD_ALL

        GroupMemoryBarrierWithGroupSync();
    }

    // --- Epilogue: compute last tile ---
    [unroll] for ( mm_k = 0; mm_k < BK; mm_k++ )
        OUTER_PRODUCT_8x4

    WRITEBACK_8x4
}

// ==================================================================
//  MatMulForwardBT -- C(M,N) = A(M,K) @ B^T, where B stored as (N,K)
//  Double-buffered with register prefetch.
// ==================================================================

#pragma kernel MatMulForwardBT
[numthreads( BN / TN, BM / TM, 1 )]
void MatMulForwardBT( uint3 gid  : SV_GroupID,
                      uint3 gtid : SV_GroupThreadID )
{
    uint threadCol = gtid.x;
    uint threadRow = gtid.y;
    uint flatIdx   = threadRow * ( BN / TN ) + threadCol;

    uint cRowBase = gid.y * BM;
    uint cColBase = gid.x * BN;

    uint microRow = threadRow * TM;
    uint microCol = threadCol * TN;

    uint mm_t, mm_k, kBase;

    DECLARE_ACC_8x4
    DECLARE_PREFETCH

    uint numTiles = ( _K + BK - 1 ) / BK;

    // --- Prologue: load first tile directly to shared ---
    PF_A_STD_ALL( 0 )
    PF_B_BT_ALL( 0 )
    ST_A_STD_ALL
    ST_B_BT_ALL
    GroupMemoryBarrierWithGroupSync();

    // --- Main loop ---
    for ( mm_t = 1; mm_t < numTiles; mm_t++ )
    {
        kBase = mm_t * BK;

        PF_A_STD_ALL( kBase )
        PF_B_BT_ALL( kBase )

        [unroll] for ( mm_k = 0; mm_k < BK; mm_k++ )
            OUTER_PRODUCT_8x4

        GroupMemoryBarrierWithGroupSync();

        ST_A_STD_ALL
        ST_B_BT_ALL

        GroupMemoryBarrierWithGroupSync();
    }

    // --- Epilogue ---
    [unroll] for ( mm_k = 0; mm_k < BK; mm_k++ )
        OUTER_PRODUCT_8x4

    WRITEBACK_8x4
}

// ==================================================================
//  MatMulForwardAT -- C(M,N) = A^T @ B, where A stored as (K,M)
//  Double-buffered with register prefetch.
// ==================================================================

#pragma kernel MatMulForwardAT
[numthreads( BN / TN, BM / TM, 1 )]
void MatMulForwardAT( uint3 gid  : SV_GroupID,
                      uint3 gtid : SV_GroupThreadID )
{
    uint threadCol = gtid.x;
    uint threadRow = gtid.y;
    uint flatIdx   = threadRow * ( BN / TN ) + threadCol;

    uint cRowBase = gid.y * BM;
    uint cColBase = gid.x * BN;

    uint microRow = threadRow * TM;
    uint microCol = threadCol * TN;

    uint mm_t, mm_k, kBase;

    DECLARE_ACC_8x4
    DECLARE_PREFETCH

    uint numTiles = ( _K + BK - 1 ) / BK;

    // --- Prologue: load first tile directly to shared ---
    PF_A_AT_ALL( 0 )
    PF_B_STD_ALL( 0 )
    ST_A_AT_ALL
    ST_B_STD_ALL
    GroupMemoryBarrierWithGroupSync();

    // --- Main loop ---
    for ( mm_t = 1; mm_t < numTiles; mm_t++ )
    {
        kBase = mm_t * BK;

        PF_A_AT_ALL( kBase )
        PF_B_STD_ALL( kBase )

        [unroll] for ( mm_k = 0; mm_k < BK; mm_k++ )
            OUTER_PRODUCT_8x4

        GroupMemoryBarrierWithGroupSync();

        ST_A_AT_ALL
        ST_B_STD_ALL

        GroupMemoryBarrierWithGroupSync();
    }

    // --- Epilogue ---
    [unroll] for ( mm_k = 0; mm_k < BK; mm_k++ )
        OUTER_PRODUCT_8x4

    WRITEBACK_8x4
}

//------------------------------------------------------------------
//  Tiled transpose — 32x32 tiles with shared memory
//  Coalesced reads and writes, +1 padding to avoid bank conflicts
//------------------------------------------------------------------

#define TRANS_TILE 32

groupshared float _transTile[ TRANS_TILE ][ TRANS_TILE + 1 ];

#pragma kernel Transpose
[numthreads( TRANS_TILE, 8, 1 )]
void Transpose( uint3 gid : SV_GroupID,
                uint3 tid : SV_GroupThreadID )
{
    uint xBlock = gid.x * TRANS_TILE;
    uint yBlock = gid.y * TRANS_TILE;

    // Declare loop counter once to avoid D3D11 HLSL scoping warnings
    uint tr_i;

    // Coalesced load -- each of 256 threads covers 4 rows
    [unroll] for ( tr_i = 0; tr_i < 4; tr_i++ )
    {
        uint row = yBlock + tid.y + tr_i * 8;
        uint col = xBlock + tid.x;
        _transTile[ tid.y + tr_i * 8 ][ tid.x ] =
            ( row < _M && col < _N ) ? _Input[ row * _N + col ] : 0.0;
    }

    GroupMemoryBarrierWithGroupSync();

    // Coalesced write -- transposed block position
    [unroll] for ( tr_i = 0; tr_i < 4; tr_i++ )
    {
        uint outRow = xBlock + tid.y + tr_i * 8;
        uint outCol = yBlock + tid.x;
        if ( outRow < _N && outCol < _M )
            _Result[ outRow * _M + outCol ] = _transTile[ tid.x ][ tid.y + tr_i * 8 ];
    }
}

// ============================================================
//  Data movement
// ============================================================

#pragma kernel Copy
[numthreads( THREADS, 1, 1 )]
void Copy( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Count ) return;
    _Dst[ _DstOffset + i ] = _Src[ _SrcOffset + i ];
}

//------------------------------------------------------------------
#pragma kernel SliceCopy
[numthreads( THREADS, 1, 1 )]
void SliceCopy( uint3 id : SV_DispatchThreadID )
{
    uint outer = id.x;
    if ( outer >= _OuterSize ) return;

    uint srcBase = outer * _SrcBlockSize + _StartOffset;
    uint dstBase = outer * _DstBlockSize;

    for ( uint i = 0; i < _InnerSize; i++ )
        _Dst[ dstBase + i ] = _Src[ srcBase + i ];
}

//------------------------------------------------------------------
#pragma kernel SliceCopyBackward
[numthreads( THREADS, 1, 1 )]
void SliceCopyBackward( uint3 id : SV_DispatchThreadID )
{
    uint outer = id.x;
    if ( outer >= _OuterSize ) return;

    uint srcBase = outer * _SrcBlockSize + _StartOffset;
    uint dstBase = outer * _DstBlockSize;

    for ( uint i = 0; i < _InnerSize; i++ )
        _Dst[ srcBase + i ] += _Src[ dstBase + i ];
}

//------------------------------------------------------------------
#pragma kernel ExpandLast
[numthreads( THREADS, 1, 1 )]
void ExpandLast( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float v = _Input[ i ];
    uint base_dst = i * _Num;
    for ( uint n = 0; n < _Num; n++ )
        _Result[ base_dst + n ] = v;
}

//------------------------------------------------------------------
#pragma kernel ExpandLastBackward
[numthreads( THREADS, 1, 1 )]
void ExpandLastBackward( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    float acc = 0.0;
    uint base_src = i * _Num;
    for ( uint n = 0; n < _Num; n++ )
        acc += _ResultGrad[ base_src + n ];
    _InputGrad[ i ] += acc;
}

//------------------------------------------------------------------
#pragma kernel Gather
[numthreads( THREADS, 1, 1 )]
void Gather( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Count ) return;

    uint srcRow = _Indices[ i ];
    uint srcStart = srcRow * _FeatureSize;
    uint dstStart = i * _FeatureSize;
    for ( uint f = 0; f < _FeatureSize; f++ )
        _Dst[ dstStart + f ] = _Src[ srcStart + f ];
}

// ============================================================
//  Utility
// ============================================================

#pragma kernel ZeroFill
[numthreads( THREADS, 1, 1 )]
void ZeroFill( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = 0.0;
}

//------------------------------------------------------------------
#pragma kernel OneFill
[numthreads( THREADS, 1, 1 )]
void OneFill( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = 1.0;
}

//------------------------------------------------------------------
#pragma kernel ValueFill
[numthreads( THREADS, 1, 1 )]
void ValueFill( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;
    _Result[ i ] = _FillValue;
}

// ============================================================
//  Adam optimizer
// ============================================================

#pragma kernel AdamStep
[numthreads( THREADS, 1, 1 )]
void AdamStep( uint3 id : SV_DispatchThreadID )
{
    uint i = id.x;
    if ( i >= _Size ) return;

    uint mi = _MomentOffset + i;
    float g = _Grad[ i ];

    float m = _Beta1 * _MBuf[ mi ] + ( 1.0 - _Beta1 ) * g;
    float v = _Beta2 * _VBuf[ mi ] + ( 1.0 - _Beta2 ) * g * g;

    _MBuf[ mi ] = m;
    _VBuf[ mi ] = v;

    float mHat = m * _InvBias1;
    float vHat = v * _InvBias2;

    _Data[ i ] -= _LR * mHat / ( sqrt( vHat ) + _Epsilon );
}
